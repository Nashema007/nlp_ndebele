{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0cf95bf8de8ccb3055486a2b9cb24cf8e60483210901898ace6cbf61a1ede4195",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time\n",
    "import math\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/Ashley/Documents/Personal/UCT/NLP/Assignment1\n"
     ]
    }
   ],
   "source": [
    "assignmentPath = os.sys.path[0]\n",
    "print(assignmentPath)\n",
    "# data_xls = pd.read_excel(assignmentPath+'GOV-ZA.50000ParallelleEnWoorde.nr.pos.full.xls', dtype=str, index_col=None)\n",
    "# data_xls.to_csv(assignmentPath+'GOV-ZA.50000ParallelleEnWoorde.nr.pos.full.csv', encoding='utf-8', index = None, header=True)\n",
    "data_df = pd.read_csv(assignmentPath+'/GOV-ZA.Toetsteks.5000ParallelleEnWoorde.nr.pos.full.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the end of a sentence and add the stop symbol\n",
    "# Remove all punctuation from the dataset \n",
    "period = data_df[ data_df['Token'] == '.' ].index\n",
    "question = data_df[ data_df['Token'] == '?' ].index\n",
    "esc_mark = data_df[ data_df['Token'] == '!' ].index\n",
    "for key, value in data_df.POS.iteritems():\n",
    "    if key in period:\n",
    "        data_df['POS'][key] = '.'\n",
    "    elif key in question:\n",
    "        data_df['POS'][key] = '.'\n",
    "    elif key in esc_mark:\n",
    "        data_df['POS'][key] = '.'\n",
    "punct = data_df[ data_df['POS'] == 'PUNCT' ].index\n",
    "data_df.drop(punct, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data and add <UNK> charector for all unknown words and default POS to V\n",
    "data_df['POS'] = data_df['POS'].fillna('<UNK>')\n",
    "data_df['Token'] = data_df['Token'].fillna('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              Token   POS\n0            Khupha     V\n1           iforomo   DEM\n2          lesibawo   DEM\n3            namkha  CONJ\n4          lifumane     V\n...             ...   ...\n4230  ngonepumelelo   ADV\n4231       enarheni   LOC\n4232       yekhethu     A\n4233              .     .\n4234          <UNK>     V\n\n[3933 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#transform dataframe to a list of tuple (word,POS)\n",
    "data_df = list(data_df.to_records(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_dev_data = train_test_split(data_df, test_size=.20,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, dev_data = train_test_split(test_dev_data, test_size=0.5,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = set(tags for words, tags in train_data)\n",
    "vocab = set(words for words, tags in train_data)"
   ]
  }
 ]
}